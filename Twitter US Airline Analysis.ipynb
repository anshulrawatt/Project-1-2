{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3343f69d",
   "metadata": {},
   "source": [
    "# Twitter Airline Sentiment Analysis, Exploratory Data Analysis and Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18073b27",
   "metadata": {},
   "source": [
    "Dataset Description: It is a record of tweets about airlines in US. Along with other information, it contains ID of Tweet, sentiment of tweer ( neutral, negative and positive), reason for negative tweet, name of airline and text of tweet. Here it is posed as a binary classififcation problem by converting neutral and positve into one category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c775daff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Import module imdb & other keras modules\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "#API to manipulate sequences of words\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#We will have three types of layers.\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Flatten\n",
    "\n",
    "#Misc\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e6e2bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display multiple commands output from a cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31838e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define some constants:\n",
    "max_vocabulary = 20000        # words\n",
    "max_len_tweet = 500          # words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1aa5d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the dataframe\n",
    "data=pd.read_csv('E:/Work & Study/MBA/T5/FA/twitter data sentiment analysis/Tweets.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b3a360fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape:\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5f8834",
   "metadata": {},
   "source": [
    "### Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "779a679c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the dataset\n",
    "X = data['text']\n",
    "y = data['airline_sentiment']\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1af3d871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'neutral': 3099, 'positive': 2363, 'negative': 9178})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b49c65",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "\n",
    "The first step when building a neural network model is getting the data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word with an integer. We'll also want to clean it up a bit.\n",
    "\n",
    "Here are the processing steps, we'll want to take:\n",
    "\n",
    "We'll want to get rid of periods and extraneous punctuation.\n",
    "We'll want to remove web address, twitter id, and digit.\n",
    "First, let's remove all punctuation. Then get all the text without the newlines and split it into individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b4c59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = '!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'\n",
    "\n",
    "# get rid of punctuation\n",
    "all_reviews = 'separator'.join(X)\n",
    "all_reviews = all_reviews.lower()\n",
    "all_text = ''.join([c for c in all_reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "reviews_split = all_text.split('separator')\n",
    "all_text = ' '.join(reviews_split)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a689ff",
   "metadata": {},
   "source": [
    "Then, we remove web address, twitter id, and digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0becd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of web address, twitter id, and digit\n",
    "new_reviews = []\n",
    "for review in reviews_split:\n",
    "    review = review.split()\n",
    "    new_text = []\n",
    "    for word in review:\n",
    "        if (word[0] != '@') & ('http' not in word) & (~word.isdigit()):\n",
    "            new_text.append(word)\n",
    "    new_reviews.append(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7317e70",
   "metadata": {},
   "source": [
    "### Encoding the Tweets\n",
    "The embedding lookup requires that we pass in integers to our network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then, we can convert each of our reviews into integers so they can be passed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28fdae80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "#use the dict to tokenize each review in reviews_split\n",
    "#store the tokenized reviews in reviews_ints\n",
    "reviews_ints = []\n",
    "for review in new_reviews:\n",
    "    reviews_ints.append([vocab_to_int[word] for word in review])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc06d11",
   "metadata": {},
   "source": [
    "Let's print out the number of unique words in the vocabulary and the contents of the first, tokenized review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce6d1fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  17213\n",
      "Tokenized review: \n",
      " [[57, 217]]\n"
     ]
    }
   ],
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))\n",
    "\n",
    "# print tokens in first review\n",
    "print('Tokenized review: \\n', reviews_ints[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a2c3374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@JetBlue yes thankfully! Catering just got here and now they are loading, but very frustrated. I was supposed to be there by 10-10:30'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[167,\n",
       " 2530,\n",
       " 1165,\n",
       " 41,\n",
       " 92,\n",
       " 141,\n",
       " 10,\n",
       " 39,\n",
       " 54,\n",
       " 35,\n",
       " 2580,\n",
       " 31,\n",
       " 151,\n",
       " 486,\n",
       " 3,\n",
       " 23,\n",
       " 390,\n",
       " 1,\n",
       " 32,\n",
       " 71,\n",
       " 102]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[8224]\n",
    "reviews_ints[8224]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd1431a",
   "metadata": {},
   "source": [
    "### Encoding the labels\n",
    "As mentioned before, our goal is to identify whether a tweet is negative or non-negative (positive or neutral). Our labels are \"positive\", \"negative\", or \"neutral. To use these labels in our network, we need to convert them to 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e12b49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1=positive, 1=neutral, 0=negative label conversion\n",
    "encoded_labels = []\n",
    "for label in y:\n",
    "    if label == 'neutral':\n",
    "        encoded_labels.append(1)\n",
    "    elif label == 'negative':\n",
    "        encoded_labels.append(0)\n",
    "    else:\n",
    "        encoded_labels.append(1)\n",
    "\n",
    "encoded_labels = np.asarray(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "835285f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 0, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1c6c6205",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check max and min length of reviews\n",
    "maxLen = 0         # Start with a low number\n",
    "minLen = 200       # Start with a high number\n",
    "for i in range(len(reviews_ints)):\n",
    "    if len(reviews_ints[i]) > maxLen:\n",
    "        maxLen = len(reviews_ints[i])\n",
    "    if len(reviews_ints[i]) < minLen :\n",
    "        minLen = len(reviews_ints[i])\n",
    "\n",
    "maxLen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8254f10b",
   "metadata": {},
   "source": [
    "### Process data\n",
    "We want to pad all sequences to max_len_review size. Reviews more in size will be truncated and less in size will be padded with zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "372d25f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad X sequences\n",
    "#And also make each inner list as one row:\n",
    "\n",
    "feature = sequence.pad_sequences(\n",
    "                                 reviews_ints,   # An array of lists where each inner\n",
    "                                            # list is a sequence, Or,\n",
    "                                            # A list of lists with each\n",
    "                                            #  list being a sequence\n",
    "                                 maxlen = 30,   # This is default\n",
    "                                 padding = 'pre'   # option: 'post'\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "71d4ca1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0, 430],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,  29,  14, 557,   4],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  44],\n",
       "       [  0,   0,   0,   0,   3, 646,  33, 505,   1, 354],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at first twenty rows\n",
    "#and first twenty columns:\n",
    "\n",
    "feature[:20,:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac4c69",
   "metadata": {},
   "source": [
    "### Training, validation, and test\n",
    "With our data in nice shape, we'll split it into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be146b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    feature, encoded_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b039f98e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0,    0,    0,    0,    0,    0,    0,  167, 2530,\n",
       "        1165,   41,   92,  141,   10,   39,   54,   35, 2580,   31,  151,\n",
       "         486,    3,   23,  390,    1,   32,   71,  102],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,  167,    3,   22,\n",
       "          62,  644, 5074,   58,   20,   76, 8032,   89,  122,  569,    9,\n",
       "          94,  101,   10,   11,  150,   47,   25,  350],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  455,   10,    3,   22,  181,  146, 2272,   10,   50,\n",
       "         231,   15,  547,  305,  593,   25,   28,  142],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,   69,   43,\n",
       "           3,   72,   24,   22,   11,  142,   54,  568, 8594,  255,  644,\n",
       "          72,   27,    2, 8595,   56,   96,  240, 1508],\n",
       "       [   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,  174,  261, 1387,  664,  799, 4267]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0:5]\n",
    "print(\"\\n\\n------------\\n\\n\")\n",
    "y_train[:4]  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a8c12",
   "metadata": {},
   "source": [
    "### Design model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "99e97b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete any earlier model \n",
    "if 'model' in locals():\n",
    "  del model\n",
    "\n",
    "#Start with a blank template:\n",
    "model = Sequential() \n",
    "\n",
    "#Add an embedding layer:\n",
    "model.add(Embedding(\n",
    "                    max_vocabulary,            # Decides number of input neurons\n",
    "                    32,                        # Decides number of neurons in hidden layer\n",
    "                    input_length= 30) # (optional) Decides how many groups of OHEs\n",
    "                                                  # are input at a time (or in sequence).\n",
    "                                                  # It also decides how many times\n",
    "                                                  #  RNN should loop around\n",
    "                                                  #    If omitted, decided autoamtically\n",
    "                                                  #     during 'model.fit()' by considering\n",
    "                                                  #       x_train.shape[1]\n",
    "                  \n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2feb4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 30, 32)            640000    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 640,000\n",
      "Trainable params: 640,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# It is instructive to see number of parameters\n",
    "#  in the summary. This tells us about the Embedding\n",
    "#   layer as being two layered network with no of neurons\n",
    "#    as max_vocabulary and output (hidden) layer with 32 neurons\n",
    "#     Note: Hidden layer has no activation function\n",
    "#            and no bias parameter:\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a67d1e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ideally we should be adding not one RNN but as many RNNs as\n",
    "#     there are timesteps ie sequence length or 'max_len_review'.\n",
    "#     But we add just one and perform internal looping. Note that\n",
    "#     internal weights and hence LSTM parameters remain same from one\n",
    "#     'timestep' to another 'timestep'. You can verify this by\n",
    "#     changing the value of max_len_review and seein that number\n",
    "#     of parameters in the model summary after adding the following\n",
    "#     do not change.\n",
    "\n",
    "model.add(\n",
    "           SimpleRNN\n",
    "                    (\n",
    "                      32,                      # Neurons at the output\n",
    "                      return_sequences = False # Make it True\n",
    "                                               # And add layer #4.4\n",
    "                    )\n",
    "          )   # Output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37e53487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 30, 32)            640000    \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 32)                2080      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 642,080\n",
      "Trainable params: 642,080\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "156d2aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cd96d57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 30, 32)            640000    \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 32)                2080      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 642,113\n",
      "Trainable params: 642,113\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Add classification layer:\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a1b10137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "#Plot model\n",
    "tf.keras.utils.plot_model(\n",
    "                          model,\n",
    "                          show_shapes=True,\n",
    "                          show_layer_names=True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9da0a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model\n",
    "model.compile(\n",
    "               loss = 'binary_crossentropy',\n",
    "               optimizer = 'rmsprop',\n",
    "               metrics = ['acc']\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "854f7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard callback\n",
    "#       We will use TensorBoard to visualize metrics \n",
    "#       including loss, accuracy etc. \n",
    "#       Create a tf.keras.callbacks.TensorBoard\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6cc1db19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "293/293 [==============================] - 5s 9ms/step - loss: 0.4949 - acc: 0.7618 - val_loss: 0.4004 - val_acc: 0.8237\n",
      "Epoch 2/5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.3380 - acc: 0.8537 - val_loss: 0.3742 - val_acc: 0.8267\n",
      "Epoch 3/5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.2571 - acc: 0.8940 - val_loss: 0.3692 - val_acc: 0.8318\n",
      "Epoch 4/5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.1804 - acc: 0.9325 - val_loss: 0.4344 - val_acc: 0.8071\n",
      "Epoch 5/5\n",
      "293/293 [==============================] - 2s 8ms/step - loss: 0.1198 - acc: 0.9568 - val_loss: 0.5246 - val_acc: 0.7909\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.22717999219894408"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5\n",
    "start = time.time()\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    batch_size = 32,             # Number of samples per gradient update\n",
    "                    validation_split = 0.2,      # Fraction of training data to be used as validation data\n",
    "                    epochs = epochs,\n",
    "                    shuffle = True,              # Shuffle training data before each epoch\n",
    "                    callbacks=[tensorboard_callback],\n",
    "                    verbose =1\n",
    "                    )\n",
    "end = time.time()\n",
    "(end-start)/60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2c32d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get x_test padded\n",
    "X_test = sequence.pad_sequences(\n",
    "                                 X_test,   # A list of lists where each inner\n",
    "                                            # list is a sequence, Or,\n",
    "                                            # An array of lists with each\n",
    "                                            #  list being a sequence\n",
    "                                 maxlen = 30,\n",
    "                                 padding = 'pre'\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d36512b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict now\n",
    "out = model.predict(X_test)\n",
    "out[out > 0.5]  = 1\n",
    "out[out <= 0.5] = 0\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b277b7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92/92 [==============================] - 0s 2ms/step - loss: 0.5768 - acc: 0.7749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5768351554870605, 0.7749316692352295]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)\n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7092933b",
   "metadata": {},
   "source": [
    "## Hugging Face Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c5e86e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (21.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->transformers) (2021.10.8)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bd6c2aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call libraries:\n",
    "# Hugging Face related:\n",
    "from transformers import pipeline\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6c1ef",
   "metadata": {},
   "source": [
    "### Classification\n",
    "Create an object to perform sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "458be815",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#     Instantiate 'pipeline' for sentiment-anaysis\n",
    "#     Once instantiated, 'classifier' object\n",
    "#     can be used for sentiment analysis:\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8681b1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading the dataframe\n",
    "data1=pd.read_csv('E:/Work & Study/MBA/T5/FA/twitter data sentiment analysis/Tweets.csv')\n",
    "data1.head()\n",
    "#Transform pandas dataframe to hugging face dataset:\n",
    "\n",
    "dataset = Dataset.from_pandas(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f5e63353",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = data1['text']\n",
    "y1 = data1['airline_sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dbc6be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = '!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~'\n",
    "\n",
    "# get rid of punctuation\n",
    "all_reviews1 = 'separator'.join(X1)\n",
    "all_reviews1 = all_reviews1.lower()\n",
    "all_text1 = ''.join([c for c in all_reviews if c not in punctuation])\n",
    "\n",
    "# split by new lines and spaces\n",
    "reviews_split1 = all_text1.split('separator')\n",
    "all_text1 = ' '.join(reviews_split1)\n",
    "\n",
    "# create a list of words\n",
    "words1 = all_text1.split()\n",
    "\n",
    "# get rid of web address, twitter id, and digit\n",
    "new_reviews1 = []\n",
    "for review in reviews_split1:\n",
    "    review = review.split()\n",
    "    new_text1 = []\n",
    "    for word in review:\n",
    "        if (word[0] != '@') & ('http' not in word) & (~word.isdigit()):\n",
    "            new_text1.append(word)\n",
    "    new_reviews1.append(new_text1)\n",
    "final_review=[]   \n",
    "for review in new_reviews1:\n",
    "    joined=\"\"\n",
    "    for word in review:\n",
    "        joined=joined+\" \"+word\n",
    "    final_review.append(joined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6cca9855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' what said',\n",
       " ' plus youve added commercials to the experience tacky',\n",
       " ' i didnt today must mean i need to take another trip']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at first few rows:\n",
    "\n",
    "final_review[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1a958e35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 15)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a sample of dataset\n",
    "#     select(range(1000)) will select top 1000 rows.\n",
    "#     Hence shuffle is a must to take a sample:\n",
    "\n",
    "sample = dataset.shuffle(seed=42).select(range(1000))\n",
    "sample.shape  # (1000, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7d391ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9899877905845642},\n",
       " {'label': 'NEGATIVE', 'score': 0.9438981413841248},\n",
       " {'label': 'NEGATIVE', 'score': 0.9987396597862244},\n",
       " {'label': 'NEGATIVE', 'score': 0.9974498152732849},\n",
       " {'label': 'NEGATIVE', 'score': 0.9996324777603149}]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classify five of the reviews:\n",
    "\n",
    "classifier(final_review[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc64fe6",
   "metadata": {},
   "source": [
    "## Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d7b3ddce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e74e2cf657d4b6a983e7196691613a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/473 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f19135e89095462981e2259e4aa2ed75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/261M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-cased-distilled-squad were not used when initializing TFDistilBertForQuestionAnswering: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-cased-distilled-squad and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d9e0337eec41b5b5a6cb994f6ad663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db638dd7611141f8bca5c0410a2b4da1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/213k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fef49d0cc948498c329ee813113346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/436k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Instantiate question-answer object:\n",
    "\n",
    "question_answerer = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6eccae65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.7554116249084473, 'start': 32, 'end': 39, 'answer': 'seattle'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let the object study context and answer question:\n",
    "\n",
    "question_answerer(\n",
    "    question=\"flights leaving dallas to which country\",\n",
    "    context= final_review[44]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2c21bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
